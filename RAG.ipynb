{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49fc17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/pymilvus/client/__init__.py:6: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/ragas/metrics/__init__.py:1: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._answer_correctness import AnswerCorrectness, answer_correctness\n",
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/ragas/metrics/__init__.py:4: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from ragas.metrics._context_entities_recall import (\n"
     ]
    }
   ],
   "source": [
    "# Load all required Libraries\n",
    "import sys, subprocess\n",
    "try:\n",
    "    import sentence_transformers  # noqa: F401\n",
    "except Exception:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", \"assignment2-rag/requirements.txt\"])  # installs deps if missing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "from pymilvus import MilvusClient, FieldSchema, CollectionSchema, DataType\n",
    "\n",
    "import evaluate as hf_evaluate\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: P\n"
     ]
    }
   ],
   "source": [
    "# test if API key is working\n",
    "import os, requests, json, time\n",
    "\n",
    "API_KEY = \"SECRET_KEY\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"SECRET_KEY\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "client_openai = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "try:\n",
    "    r = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\":\"user\",\"content\":\"ping\"}],\n",
    "        max_tokens=1,\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"OK:\", r.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"ERROR:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275ab245",
   "metadata": {},
   "source": [
    "# Read Passages from the Datasets and Drop rows if they are NA or empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff0e11a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>passage</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Uruguay (official full name in  ; pron.  , Eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It is bordered by Brazil to the north, by Arge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Montevideo was founded by the Spanish in the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The economy is largely based in agriculture (m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>According to Transparency International, Urugu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              passage\n",
       "id                                                   \n",
       "0   Uruguay (official full name in  ; pron.  , Eas...\n",
       "1   It is bordered by Brazil to the north, by Arge...\n",
       "2   Montevideo was founded by the Spanish in the e...\n",
       "3   The economy is largely based in agriculture (m...\n",
       "4   According to Transparency International, Urugu..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passages = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/passages.parquet/part.0.parquet\")\n",
    "\n",
    "# drop rows if they are NA or empty\n",
    "passages = passages.dropna(subset=[\"passage\"])\n",
    "passages = passages[passages[\"passage\"].astype(str).str.strip().ne(\"\")]\n",
    "\n",
    "print(passages.shape)\n",
    "passages_reset = passages.reset_index()\n",
    "passages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45249cd2",
   "metadata": {},
   "source": [
    "# Do EDA on the passage dataset\n",
    "- You can try to find the maximum and minimum length of the passages before indexing (just a direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa0d0a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_passages': 3200, 'min_len': 1, 'max_len': 2515, 'avg_len': 389.848125, 'median_len': 299.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['In some beetles, the ability to fly has been lost. These include the ground beetles (family Carabidae) and some \"true weevils\" (family Curculionidae), but also some desert and cave-dwelling species of other families. Many of these species have the two elytra fused together, forming a solid shield over the abdomen. In a few families, both the ability to fly and the elytra have been lost, with the best known example being the glow-worms of the family Phengodidae, in which the females are larviform throughout their lives.',\n",
       " 'The name \"Qatar\" may derive from the same Arabic root as qatura which means \"to exude.\"  The word Qatura traces to the Arabic qatran meaning \"tar\" or \"resin\", which relates to the country\\'s rich resources in petroleum and natural gas.  Adrian Room, Placenames of the World (1997) McFarland and Company.',\n",
       " \"President Woodrow Wilson articulated what became known as the Fourteen Points before Congress on January 8, 1918.  The Points were the only war aims clearly expressed by any belligerent nation and thus became the basis for the Treaty of Versailles following World War I.  The speech was highly idealistic, translating Wilson's progressive domestic policy of democracy, self-determination, open agreements, and free trade into the international realm.  It also made several suggestions for specific disputes in Europe on the recommendation of Wilson's foreign policy advisor, Colonel Edward M. House, and his team of 150 advisors known as â\\x80\\x9cThe Inquiry.â\\x80\\x9d  The points were:\",\n",
       " '|}',\n",
       " '* 3. If the plane is straight across, the section cut out will be a circle.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code for EDA\n",
    "passages_series = passages[\"passage\"].astype(str)\n",
    "lengths = passages_series.str.len()\n",
    "print({\n",
    "    \"num_passages\": len(passages_series),\n",
    "    \"min_len\": int(lengths.min()),\n",
    "    \"max_len\": int(lengths.max()),\n",
    "    \"avg_len\": float(lengths.mean()),\n",
    "    \"median_len\": float(lengths.median()),\n",
    "})\n",
    "passages_series.sample(5, random_state=42).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6b32f3",
   "metadata": {},
   "source": [
    "# Tokenize Text and Generate Embeddings using Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f39adefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 50/50 [00:11<00:00,  4.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "# Encode Text\n",
    "passages_reset = passages.reset_index()  # ensure 'id' is a column\n",
    "passage_texts = passages_reset[\"passage\"].astype(str).tolist()\n",
    "embeddings = embedding_model.encode(\n",
    "    passage_texts,\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354e711",
   "metadata": {},
   "source": [
    "# Create Milvus Client and Insert your Embeddings to your DB\n",
    "- Make sure you define a schema for your collection (Points will be deducted if you fail to define a proper schema with ids, passage text, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ccfdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define every column of your schema\n",
    "\n",
    "# Infer embedding dimension from computed embeddings\n",
    "embedding_dim = int(embeddings.shape[1])\n",
    "\n",
    "id_ = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False)\n",
    "passage = FieldSchema(name=\"passage\", dtype=DataType.VARCHAR, max_length=8192)\n",
    "embedding = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13b4c62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = CollectionSchema(fields=[id_, passage, embedding], description=\"RAG Mini Wikipedia collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed3c24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created: rag_mini\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import MilvusClient\n",
    "DB_PATH = \"rag_wikipedia_mini_v3.db\"  # new file to avoid lock\n",
    "client = MilvusClient(DB_PATH)\n",
    "\n",
    "# Create the Collection with Collection Name = \"rag_mini\". Make sure you define the schema variable while creating the collection\n",
    "try:\n",
    "    if client.has_collection(\"rag_mini\"):\n",
    "        client.drop_collection(\"rag_mini\")\n",
    "except Exception as e:\n",
    "    print(\"Drop existing rag_mini failed:\", e)\n",
    "\n",
    "try:\n",
    "    client.create_collection(\n",
    "        collection_name=\"rag_mini\",\n",
    "        schema=schema,\n",
    "        shard_num=1,\n",
    "    )\n",
    "    print(\"Collection created: rag_mini\")\n",
    "except Exception as e:\n",
    "    print(f\"Create collection result: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190b067a",
   "metadata": {},
   "source": [
    "**Convert your Pandas Dataframe to a list of dictionaries**\n",
    "- The Dictionary at least have 3 keys [id, passage, embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b60520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200 rows ready for insert\n"
     ]
    }
   ],
   "source": [
    "# Convert Pandas DataFrame + embeddings to Milvus row dicts\n",
    "ids = passages_reset[\"id\"].astype(int).tolist()\n",
    "rag_data = [\n",
    "    {\n",
    "        \"id\": int(idx),\n",
    "        \"passage\": str(text),\n",
    "        \"embedding\": emb.astype(float).tolist(),\n",
    "    }\n",
    "    for idx, text, emb in zip(ids, passage_texts, embeddings)\n",
    "]\n",
    "print(len(rag_data), \"rows ready for insert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ba91d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted: 3200\n"
     ]
    }
   ],
   "source": [
    "# Code to insert the data to your DB\n",
    "try:\n",
    "    res = client.insert(collection_name=\"rag_mini\", data=rag_data)\n",
    "    print(\"Inserted:\", res.get(\"insert_count\", res))\n",
    "except Exception as e:\n",
    "    print(f\"Insert result: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba2a1a",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "- Do a Sanity Check on your database \n",
    "\n",
    "**Do not delete the below line during your submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e73b817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity count: 3200\n",
      "Collection schema: {'collection_name': 'rag_mini', 'auto_id': False, 'num_shards': 0, 'description': 'RAG Mini Wikipedia collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.INT64: 5>, 'params': {}, 'is_primary': True}, {'field_id': 101, 'name': 'passage', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 8192}}, {'field_id': 102, 'name': 'embedding', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 384}}], 'aliases': [], 'collection_id': 0, 'consistency_level': 0, 'properties': {}, 'num_partitions': 0, 'enable_dynamic_field': False}\n"
     ]
    }
   ],
   "source": [
    "print(\"Entity count:\", client.get_collection_stats(\"rag_mini\")[\"row_count\"])\n",
    "print(\"Collection schema:\", client.describe_collection(\"rag_mini\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87920ab",
   "metadata": {},
   "source": [
    "# Steps to Fetch Results\n",
    "- Read the Question Dataset\n",
    "- Clean the Question Dataset if necessary (Drop Questions with NaN etc.)\n",
    "- Convert Each Query to a Vector Embedding (Use the same embedding model you used to embed your document)\n",
    "- Try for a Single Question First\n",
    "- Load Collection into Memory after creating Index for Search on your embedding field (This is an essential step before you can search in your db)\n",
    "- Search and Fetch Top N Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da659821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(918, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question     answer\n",
       "id                                                              \n",
       "0   Was Abraham Lincoln the sixteenth President of...        yes\n",
       "2   Did Lincoln sign the National Banking Act of 1...        yes\n",
       "4                    Did his mother die of pneumonia?         no\n",
       "6       How many long was Lincoln's formal education?  18 months\n",
       "8        When did Lincoln begin his political career?       1832"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queries = pd.read_parquet(\"hf://datasets/rag-datasets/rag-mini-wikipedia/data/test.parquet/part.0.parquet\")\n",
    "\n",
    "queries = queries.dropna(subset=[\"question\"])\n",
    "queries = queries[queries[\"question\"].astype(str).str.strip().ne(\"\")]\n",
    "\n",
    "print(queries.shape)\n",
    "\n",
    "queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "849c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 384)\n"
     ]
    }
   ],
   "source": [
    "query = queries.iloc[0][\"question\"]\n",
    "\n",
    "query_embedding = embedding_model.encode([str(query)], normalize_embeddings=True)\n",
    "\n",
    "print(query_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c1383",
   "metadata": {},
   "source": [
    "#### Create Index on the embedding column on your DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22563d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created on embedding\n",
      "Collection loaded into memory\n"
     ]
    }
   ],
   "source": [
    "index_params = MilvusClient.prepare_index_params()\n",
    "\n",
    "# Add an index on the embedding field\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"AUTOINDEX\",  # or IVF_FLAT/HNSW if desired\n",
    "    metric_type=\"IP\",        # cosine works when normalized; use IP for speed\n",
    "    params={}\n",
    ")\n",
    "\n",
    "# Create the index\n",
    "try:\n",
    "    client.create_index(collection_name=\"rag_mini\", index_params=index_params)\n",
    "    print(\"Index created on embedding\")\n",
    "except Exception as e:\n",
    "    print(f\"Index creation result: {e}\")\n",
    "\n",
    "# Load collection into memory (required for search)\n",
    "try:\n",
    "    client.load_collection(collection_name=\"rag_mini\")\n",
    "    print(\"Collection loaded into memory\")\n",
    "except Exception as e:\n",
    "    print(f\"Load collection result: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e60b217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 50/50 [00:48<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created: rag_mini_mpnet\n",
      "Inserted mpnet: 3200\n",
      "rag_mini_mpnet loaded\n"
     ]
    }
   ],
   "source": [
    "# Build second collection with all-mpnet-base-v2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "MPNET_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "embedding_model_mpnet = SentenceTransformer(MPNET_MODEL_NAME)\n",
    "mpnet_embeddings = embedding_model_mpnet.encode(\n",
    "    passage_texts,\n",
    "    batch_size=64,\n",
    "    convert_to_numpy=True,\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True,\n",
    ")\n",
    "embedding_dim_mpnet = int(mpnet_embeddings.shape[1])\n",
    "\n",
    "id2 = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False)\n",
    "passage2 = FieldSchema(name=\"passage\", dtype=DataType.VARCHAR, max_length=8192)\n",
    "embedding2 = FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim_mpnet)\n",
    "schema_mpnet = CollectionSchema(fields=[id2, passage2, embedding2], description=\"RAG Mini Wikipedia mpnet\")\n",
    "\n",
    "try:\n",
    "    if client.has_collection(\"rag_mini_mpnet\"):\n",
    "        client.drop_collection(\"rag_mini_mpnet\")\n",
    "except Exception as e:\n",
    "    print(\"Drop existing rag_mini_mpnet failed:\", e)\n",
    "\n",
    "try:\n",
    "    client.create_collection(collection_name=\"rag_mini_mpnet\", schema=schema_mpnet, shard_num=1)\n",
    "    print(\"Collection created: rag_mini_mpnet\")\n",
    "except Exception as e:\n",
    "    print(\"Create collection (mpnet):\", e)\n",
    "\n",
    "rag_data_mpnet = [\n",
    "    {\"id\": int(idx), \"passage\": str(text), \"embedding\": emb.astype(float).tolist()}\n",
    "    for idx, text, emb in zip(ids, passage_texts, mpnet_embeddings)\n",
    "]\n",
    "res = client.insert(collection_name=\"rag_mini_mpnet\", data=rag_data_mpnet)\n",
    "print(\"Inserted mpnet:\", res.get(\"insert_count\", res))\n",
    "\n",
    "index_params2 = MilvusClient.prepare_index_params()\n",
    "index_params2.add_index(field_name=\"embedding\", index_type=\"AUTOINDEX\", metric_type=\"IP\", params={})\n",
    "try:\n",
    "    client.create_index(collection_name=\"rag_mini_mpnet\", index_params=index_params2)\n",
    "    client.load_collection(collection_name=\"rag_mini_mpnet\")\n",
    "    print(\"rag_mini_mpnet loaded\")\n",
    "except Exception as e:\n",
    "    print(\"Index/load (mpnet):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "664364e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: [\"[{'id': 288, 'distance': 0.7095188498497009, 'entity': {}}, {'id': 278, 'distance': 0.5840359926223755, 'entity': {}}, {'id': 698, 'distance': 0.5568779110908508, 'entity': {}}, {'id': 2228, 'distance': 0.5566980838775635, 'entity': {}}, {'id': 319, 'distance': 0.5500738024711609, 'entity': {}}, {'id': 390, 'distance': 0.548395037651062, 'entity': {}}, {'id': 1813, 'distance': 0.5443781614303589, 'entity': {}}, {'id': 317, 'distance': 0.5384628176689148, 'entity': {}}, {'id': 392, 'distance': 0.538284182548523, 'entity': {}}, {'id': 289, 'distance': 0.530716061592102, 'entity': {}}]\"] , extra_info: {'cost': 0}\n",
      "Top IDs: [288, 278, 698, 2228, 319]\n"
     ]
    }
   ],
   "source": [
    "# Search the db with your query embedding\n",
    "search_res = client.search(\n",
    "    collection_name=\"rag_mini\",\n",
    "    data=query_embedding.tolist(),\n",
    "    anns_field=\"embedding\",\n",
    "    limit=10,\n",
    "    search_params={\"metric_type\": \"IP\"}\n",
    ")\n",
    "print(search_res)\n",
    "# Extract ids and distances for top-k\n",
    "hits = search_res[0]\n",
    "retrieved_ids = [hit[\"id\"] for hit in hits]\n",
    "retrieved_scores = [hit[\"distance\"] for hit in hits]\n",
    "print(\"Top IDs:\", retrieved_ids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe7a24",
   "metadata": {},
   "source": [
    "## Now get the Context \n",
    "- Initially use the first passage ONLY as your context\n",
    "- In Later Experiments, you must try at least 2 different passage selection strategies (Top 3 / Top 5 / Top 10) and pass to your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eea31d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young Abraham Lincoln ...\n"
     ]
    }
   ],
   "source": [
    "# Use first retrieved passage as context\n",
    "first_id = int(retrieved_ids[0])\n",
    "context = passages_reset.loc[passages_reset[\"id\"] == first_id, \"passage\"].values[0]\n",
    "print(context[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3b68ab",
   "metadata": {},
   "source": [
    "**Develop your Prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca4674fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " Context: Young Abraham Lincoln: \n",
      " Question: Was Abraham Lincoln the sixteenth President of the United States? \n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\n",
    "\n",
    "prompt = f\"\"\"{system_prompt} \\n Context: {context}: \\n Question: {query} \"\"\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7bb4e9",
   "metadata": {},
   "source": [
    "# RAG Response for a Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e28172e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "GEN_MODEL = \"google/flan-t5-small\"\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
    "hf_lm = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL)\n",
    "\n",
    "def generate_answer(prompt_text: str, max_new_tokens: int = 128) -> str:\n",
    "    inputs = hf_tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        output_ids = hf_lm.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return hf_tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dff0c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: no\n"
     ]
    }
   ],
   "source": [
    "# Generate answer\n",
    "answer = generate_answer(prompt)\n",
    "print(\"Answer:\", answer)\n",
    "\n",
    "# Decode and extract answer.\n",
    "# Already decoded above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9023b43c",
   "metadata": {},
   "source": [
    "# Generate Responses for all the Queries in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cc262ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 918/918 [01:32<00:00,  9.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>retrieved_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Did Lincoln sign the National Banking Act of 1...</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did his mother die of pneumonia?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many long was Lincoln's formal education?</td>\n",
       "      <td>18 months</td>\n",
       "      <td>18 months</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>When did Lincoln begin his political career?</td>\n",
       "      <td>1832</td>\n",
       "      <td>1832</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question     answer pred_answer  \\\n",
       "id                                                                             \n",
       "0   Was Abraham Lincoln the sixteenth President of...        yes          no   \n",
       "2   Did Lincoln sign the National Banking Act of 1...        yes         yes   \n",
       "4                    Did his mother die of pneumonia?         no          no   \n",
       "6       How many long was Lincoln's formal education?  18 months   18 months   \n",
       "8        When did Lincoln begin his political career?       1832        1832   \n",
       "\n",
       "    retrieved_id  \n",
       "id                \n",
       "0            288  \n",
       "2            360  \n",
       "4            262  \n",
       "6            287  \n",
       "8            289  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Batch inference over queries using top-1 context\n",
    "clean_queries = queries.dropna(subset=[\"question\"]).copy()\n",
    "clean_queries[\"question\"] = clean_queries[\"question\"].astype(str)\n",
    "\n",
    "pred_answers = []\n",
    "retrieved_first_ids = []\n",
    "for q in tqdm(clean_queries[\"question\"].tolist()):\n",
    "    q_emb = embedding_model.encode([q], normalize_embeddings=True)\n",
    "    sres = client.search(\n",
    "        collection_name=\"rag_mini\",\n",
    "        data=q_emb.tolist(),\n",
    "        anns_field=\"embedding\",\n",
    "        limit=10,\n",
    "        search_params={\"metric_type\": \"IP\"}\n",
    "    )\n",
    "    top_id = int(sres[0][0][\"id\"]) if len(sres) > 0 and len(sres[0]) > 0 else -1\n",
    "    retrieved_first_ids.append(top_id)\n",
    "    ctx = passages_reset.loc[passages_reset[\"id\"] == top_id, \"passage\"].values\n",
    "    ctx_text = ctx[0] if len(ctx) > 0 else \"\"\n",
    "    sys_prompt = \"You are a helpful assistant. Answer concisely using the provided context. If unknown, say you don't know.\"\n",
    "    prompt_i = f\"{sys_prompt}\\nContext: {ctx_text}\\nQuestion: {q}\"\n",
    "    pred = generate_answer(prompt_i)\n",
    "    pred_answers.append(pred)\n",
    "\n",
    "batch_df = clean_queries[[\"question\", \"answer\"]].copy()\n",
    "batch_df[\"pred_answer\"] = pred_answers\n",
    "batch_df[\"retrieved_id\"] = retrieved_first_ids\n",
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1b77e",
   "metadata": {},
   "source": [
    "# Finding out the Basic QA Metrics (F1 score, EM score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "357b1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Different prompting strategies comparison (top-1, same retriever)\n",
    "# prompts = [\n",
    "#     \"Answer concisely using only the context. If unknown, say you don't know.\\nContext: {ctx}\\nQuestion: {q}\",\n",
    "#     \"You are a strict QA system. Use only the context; if missing, say 'I don't know'.\\nContext: {ctx}\\nQuestion: {q}\",\n",
    "#     \"Provide a short, direct answer based on the context only.\\nContext: {ctx}\\nQuestion: {q}\",\n",
    "# ]\n",
    "\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# tmp_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# def eval_prompt(template):\n",
    "#     preds = []\n",
    "#     refs = clean_queries[\"answer\"].astype(str).tolist()\n",
    "#     for q in clean_queries[\"question\"].astype(str).tolist():\n",
    "#         q_emb = tmp_model.encode([q], normalize_embeddings=True)\n",
    "#         sres = client.search(collection_name=\"rag_mini\", data=q_emb.tolist(), anns_field=\"embedding\", limit=1, search_params={\"metric_type\": \"IP\"})\n",
    "#         pid = int(sres[0][0][\"id\"])\n",
    "#         ctx = passages_reset.loc[passages_reset[\"id\"]==pid,\"passage\"].values[0]\n",
    "#         pred = generate_answer(template.format(ctx=ctx, q=q))\n",
    "#         preds.append(pred)\n",
    "#     em_ = sum(exact_match_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "#     f1_ = sum(f1_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "#     return em_, f1_\n",
    "\n",
    "# prompt_scores = [eval_prompt(t) for t in prompts]\n",
    "# print(prompt_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c02f1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exact_match': 41.50326797385621, 'f1': 48.547767028825}\n"
     ]
    }
   ],
   "source": [
    "# Offline SQuAD-style EM/F1\n",
    "\n",
    "import re, string, json\n",
    "\n",
    "def normalize_answer(s: str) -> str:\n",
    "    def remove_articles(text): return re.sub(r\"\\b(a|an|the)\\b\", \" \", text, flags=re.IGNORECASE)\n",
    "    def remove_punc(text): return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    def white_space_fix(text): return \" \".join(text.split())\n",
    "    return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "\n",
    "def exact_match_score(pred: str, truth: str) -> int:\n",
    "    return int(normalize_answer(pred) == normalize_answer(truth))\n",
    "\n",
    "def f1_score(pred: str, truth: str) -> float:\n",
    "    p = normalize_answer(pred).split(); t = normalize_answer(truth).split()\n",
    "    if not p and not t: return 1.0\n",
    "    if not p or not t: return 0.0\n",
    "    common = {}\n",
    "    for tok in set(p):\n",
    "        common[tok] = min(p.count(tok), t.count(tok))\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0: return 0.0\n",
    "    precision = num_same / len(p); recall = num_same / len(t)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "preds = batch_df[\"pred_answer\"].astype(str).tolist()\n",
    "refs = batch_df[\"answer\"].astype(str).tolist()\n",
    "\n",
    "em = sum(exact_match_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "f1 = sum(f1_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "\n",
    "print({\"exact_match\": em, \"f1\": f1})\n",
    "\n",
    "with open(\"naive_results.json\", \"w\") as f:\n",
    "    json.dump({\"em\": em, \"f1\": f1}, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2072ac24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'em_mean': 41.50326797385621, 'em_ci': (0.38344226579520696, 0.44937363834422656), 'f1_mean': 48.54776702882495, 'f1_ci': (0.45592722971165917, 0.5147832400573178)}\n",
      "error_type\n",
      "other             506\n",
      "correct           309\n",
      "yn_mismatch        77\n",
      "number_missing     26\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>pred_answer</th>\n",
       "      <th>error_type</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Was Abraham Lincoln the sixteenth President of...</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yn_mismatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What did The Legal Tender Act of 1862 establish?</td>\n",
       "      <td>the United States Note, the first paper curren...</td>\n",
       "      <td>United States Note</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Who suggested Lincoln grow a beard?</td>\n",
       "      <td>11-year-old Grace Bedell</td>\n",
       "      <td>Grace Bedell</td>\n",
       "      <td>number_missing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>When did the Gettysburg address argue that Ame...</td>\n",
       "      <td>1776</td>\n",
       "      <td>1789</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Which county was Lincoln born in?</td>\n",
       "      <td>Hardin County</td>\n",
       "      <td>Springfield</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>When did Lincoln first serve as President?</td>\n",
       "      <td>March 4, 1861</td>\n",
       "      <td>1861</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Who assassinated Lincoln?</td>\n",
       "      <td>John Wilkes Booth</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Who was the general in charge at the Battle of...</td>\n",
       "      <td>General McClellan</td>\n",
       "      <td>Ambrose Burnside</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Why did Lincoln issue the Emancipation Proclam...</td>\n",
       "      <td>To free slaves</td>\n",
       "      <td>freed slaves in territories not under Union co...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Was Lincoln chosen as a presidential candidate...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Lincoln was eventually chosen as the Republica...</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "id                                                      \n",
       "0   Was Abraham Lincoln the sixteenth President of...   \n",
       "10   What did The Legal Tender Act of 1862 establish?   \n",
       "12                Who suggested Lincoln grow a beard?   \n",
       "14  When did the Gettysburg address argue that Ame...   \n",
       "24                  Which county was Lincoln born in?   \n",
       "26         When did Lincoln first serve as President?   \n",
       "28                          Who assassinated Lincoln?   \n",
       "32  Who was the general in charge at the Battle of...   \n",
       "34  Why did Lincoln issue the Emancipation Proclam...   \n",
       "41  Was Lincoln chosen as a presidential candidate...   \n",
       "\n",
       "                                               answer  \\\n",
       "id                                                      \n",
       "0                                                 yes   \n",
       "10  the United States Note, the first paper curren...   \n",
       "12                           11-year-old Grace Bedell   \n",
       "14                                               1776   \n",
       "24                                      Hardin County   \n",
       "26                                      March 4, 1861   \n",
       "28                                  John Wilkes Booth   \n",
       "32                                  General McClellan   \n",
       "34                                    To free slaves    \n",
       "41                                                Yes   \n",
       "\n",
       "                                          pred_answer      error_type  \n",
       "id                                                                     \n",
       "0                                                  no     yn_mismatch  \n",
       "10                                 United States Note           other  \n",
       "12                                       Grace Bedell  number_missing  \n",
       "14                                               1789           other  \n",
       "24                                        Springfield           other  \n",
       "26                                               1861           other  \n",
       "28                                    Abraham Lincoln           other  \n",
       "32                                   Ambrose Burnside           other  \n",
       "34  freed slaves in territories not under Union co...           other  \n",
       "41  Lincoln was eventually chosen as the Republica...           other  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 95% CI + failure analysis for naive batch_df (repeat for enhanced if needed)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def bootstrap_ci(vals, n_boot=500, alpha=0.05, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    vals = np.array(vals, dtype=float)\n",
    "    boots = [np.mean(vals[rng.integers(0, len(vals), len(vals))]) for _ in range(n_boot)]\n",
    "    lo = float(np.percentile(boots, 100*alpha/2))\n",
    "    hi = float(np.percentile(boots, 100*(1-alpha/2)))\n",
    "    return lo, hi\n",
    "\n",
    "per_em = [int(exact_match_score(p, r)) for p, r in zip(batch_df[\"pred_answer\"], batch_df[\"answer\"])]\n",
    "per_f1 = [f1_score(p, r) for p, r in zip(batch_df[\"pred_answer\"], batch_df[\"answer\"])]\n",
    "em_ci = bootstrap_ci(per_em)\n",
    "f1_ci = bootstrap_ci(per_f1)\n",
    "print({\"em_mean\": float(np.mean(per_em))*100, \"em_ci\": em_ci,\n",
    "       \"f1_mean\": float(np.mean(per_f1))*100, \"f1_ci\": f1_ci})\n",
    "\n",
    "def error_type(pred, ref):\n",
    "    p, r = str(pred).strip().lower(), str(ref).strip().lower()\n",
    "    if p == r: return \"correct\"\n",
    "    if r in [\"yes\",\"no\"] and p in [\"yes\",\"no\"] and p != r: return \"yn_mismatch\"\n",
    "    if any(ch.isdigit() for ch in r) and not any(ch.isdigit() for ch in p): return \"number_missing\"\n",
    "    return \"other\"\n",
    "\n",
    "fa_df = batch_df.copy()\n",
    "fa_df[\"error_type\"] = [error_type(p, r) for p, r in zip(fa_df[\"pred_answer\"], fa_df[\"answer\"])]\n",
    "print(fa_df[\"error_type\"].value_counts())\n",
    "fa_df[fa_df[\"error_type\"]!=\"correct\"][[\"question\",\"answer\",\"pred_answer\",\"error_type\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef21cbf",
   "metadata": {},
   "source": [
    "# Advanced Evaluation using RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d322cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = {\n",
    "    \"question\": batch_df[\"question\"].astype(str).tolist(),\n",
    "    \"answer\": batch_df[\"pred_answer\"].astype(str).tolist(),  # generated answer\n",
    "    \"contexts\": [[ctx] for ctx in batch_df[\"retrieved_id\"].apply(\n",
    "        lambda rid: passages_reset.loc[passages_reset[\"id\"]==rid, \"passage\"].values[0]\n",
    "        if (passages_reset[\"id\"]==rid).any() else \"\"\n",
    "    ).tolist()],\n",
    "    \"ground_truth\": batch_df[\"answer\"].astype(str).tolist(),  # reference answer\n",
    "}\n",
    "dataset = Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7dd6546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 10730.69 examples/s]\n",
      "Evaluating:  34%|███▍      | 136/400 [00:25<00:44,  5.90it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  60%|██████    | 242/400 [00:40<00:30,  5.15it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  66%|██████▌   | 263/400 [00:43<00:18,  7.46it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  69%|██████▉   | 277/400 [00:45<00:15,  8.01it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  90%|█████████ | 360/400 [01:05<00:10,  3.65it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  91%|█████████ | 363/400 [01:07<00:18,  1.95it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  92%|█████████▎| 370/400 [01:10<00:16,  1.79it/s]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 400/400 [01:22<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ragas_naive.csv with 100 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which property did James Monroe sell in 1817?</td>\n",
       "      <td>Highland Plantation</td>\n",
       "      <td>[Monroe had racked up many debts during his ye...</td>\n",
       "      <td>Monroe Hill on the grounds of the University o...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.891108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Was it a two-sentence description that complet...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>[Immediately after Lee's surrender, Grant had ...</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.837006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is an otter's den called?</td>\n",
       "      <td>a holt</td>\n",
       "      <td>[An otter's den is called a holt.  Male otters...</td>\n",
       "      <td>Holt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Who dismantled partisan and sectional coalitio...</td>\n",
       "      <td>Wilson dismantled partisan and sectional coali...</td>\n",
       "      <td>[The longest section of Congressional Governme...</td>\n",
       "      <td>many congressmen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.908237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Did James Monroe fight in the Continental Army?</td>\n",
       "      <td>yes</td>\n",
       "      <td>[* Monroe was (arguably) the last president to...</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.957436</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0      Which property did James Monroe sell in 1817?   \n",
       "1  Was it a two-sentence description that complet...   \n",
       "2                     What is an otter's den called?   \n",
       "3  Who dismantled partisan and sectional coalitio...   \n",
       "4    Did James Monroe fight in the Continental Army?   \n",
       "\n",
       "                                              answer  \\\n",
       "0                                Highland Plantation   \n",
       "1                                                Yes   \n",
       "2                                             a holt   \n",
       "3  Wilson dismantled partisan and sectional coali...   \n",
       "4                                                yes   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Monroe had racked up many debts during his ye...   \n",
       "1  [Immediately after Lee's surrender, Grant had ...   \n",
       "2  [An otter's den is called a holt.  Male otters...   \n",
       "3  [The longest section of Congressional Governme...   \n",
       "4  [* Monroe was (arguably) the last president to...   \n",
       "\n",
       "                                        ground_truth  faithfulness  \\\n",
       "0  Monroe Hill on the grounds of the University o...           0.0   \n",
       "1                                                yes           1.0   \n",
       "2                                               Holt           1.0   \n",
       "3                                   many congressmen           0.0   \n",
       "4                                                yes           0.0   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.891108                0.0             0.0  \n",
       "1          0.837006                1.0             1.0  \n",
       "2          1.000000                1.0             1.0  \n",
       "3          0.908237                0.0             0.0  \n",
       "4          0.957436                1.0             1.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAGAs single-pass minimal eval — combine 4 metrics with tight budgets\n",
    "import os, numpy as np\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.run_config import RunConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# adjust here to fit your rate limit\n",
    "K = min(100, len(dataset[\"question\"]))     # set 30 if you still hit 429\n",
    "MAX_CHARS = 600                        \n",
    "MAX_TOKENS = 128                          # set 128 if needed\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = rng.choice(len(dataset[\"question\"]), size=K, replace=False)\n",
    "subset = dataset.select(idx.tolist())\n",
    "\n",
    "def _truncate(row, max_chars=MAX_CHARS):\n",
    "    ctxs = row.get(\"contexts\", [])\n",
    "    row[\"contexts\"] = [ctxs[0][:max_chars]] if ctxs else [\"\"]\n",
    "    return row\n",
    "subset = subset.map(_truncate)\n",
    "\n",
    "ragas_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=MAX_TOKENS, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "rc = RunConfig(timeout=120)\n",
    "\n",
    "metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "res = ragas_evaluate(subset, metrics=metrics, llm=ragas_llm, run_config=rc)\n",
    "df = res.to_pandas()\n",
    "df.to_csv(\"ragas_naive.csv\", index=False)\n",
    "print(f\"Saved ragas_naive.csv with {len(df)} rows\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f6620e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cross-Encoder for reranking\n",
    "from sentence_transformers import CrossEncoder\n",
    "RERANK_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANK_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8043c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced pipeline: mpnet embeddings + CrossEncoder reranking (top-3),\n",
    "# low-budget generation and RAGAs evaluation\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from typing import List, Tuple\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate as ragas_evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "from ragas.run_config import RunConfig\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1) Reranker helper with confidence\n",
    "\n",
    "def rerank_with_confidence(query: str,\n",
    "                           candidate_ids: List[int],\n",
    "                           candidate_texts: List[str]) -> Tuple[List[float], List[float], List[Tuple[int, float]]]:\n",
    "    pairs = [(query, t) for t in candidate_texts]\n",
    "    scores = reranker.predict(pairs).tolist()\n",
    "    # Softmax for normalized confidence over candidates\n",
    "    scores_np = np.array(scores, dtype=float)\n",
    "    exps = np.exp(scores_np - np.max(scores_np))\n",
    "    probs = exps / (exps.sum() if exps.sum() > 0 else 1.0)\n",
    "    id_prob_list = list(zip(candidate_ids, probs.astype(float).tolist()))\n",
    "    return scores, probs.astype(float).tolist(), id_prob_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2369f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Build enhanced answers on a modest subset to keep runtime small\n",
    "TOP_K = 1\n",
    "CAND_LIMIT = 30\n",
    "MAX_NEW_TOKENS = 24\n",
    "SAMPLE_N = min(100, len(clean_queries))  # meets the >=100 guideline, runs fast\n",
    "\n",
    "# Ensure mpnet model is available (created earlier); fallback if needed\n",
    "try:\n",
    "    embedding_model_mpnet  # type: ignore  # noqa: F821\n",
    "except Exception:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    embedding_model_mpnet = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "pred_answers_enh, contexts_list_enh, confidences = [], [], []\n",
    "subset_q = clean_queries.head(SAMPLE_N).copy()\n",
    "\n",
    "for q in subset_q[\"question\"].astype(str).tolist():\n",
    "    q_emb = embedding_model_mpnet.encode([q], normalize_embeddings=True)\n",
    "    sres = client.search(\n",
    "        collection_name=\"rag_mini_mpnet\",\n",
    "        data=q_emb.tolist(),\n",
    "        anns_field=\"embedding\",\n",
    "        limit=CAND_LIMIT,\n",
    "        search_params={\"metric_type\": \"IP\"}\n",
    "    )\n",
    "    hits_local = sres[0] if len(sres) > 0 else []\n",
    "    cand_ids = [int(h[\"id\"]) for h in hits_local]\n",
    "    cand_texts = [\n",
    "        passages_reset.loc[passages_reset[\"id\"] == cid, \"passage\"].values[0]\n",
    "        for cid in cand_ids\n",
    "    ] if len(cand_ids) > 0 else []\n",
    "\n",
    "    if len(cand_ids) == 0:\n",
    "        contexts = [\"\"]\n",
    "        conf_score = 0.0\n",
    "    else:\n",
    "        _, _, id_prob_list = rerank_with_confidence(q, cand_ids, cand_texts)\n",
    "        sorted_by_prob = sorted(id_prob_list, key=lambda x: x[1], reverse=True)\n",
    "        topk_ids = [cid for cid, _ in sorted_by_prob[:TOP_K]]\n",
    "        contexts = [\n",
    "            passages_reset.loc[passages_reset[\"id\"] == cid, \"passage\"].values[0]\n",
    "            for cid in topk_ids\n",
    "        ]\n",
    "        conf_score = float(np.mean([p for _, p in sorted_by_prob[:TOP_K]])) if len(sorted_by_prob) > 0 else 0.0\n",
    "\n",
    "    ctx_text = \"\\n\\n\".join(contexts)\n",
    "    sys_prompt = \"You are a helpful assistant. Answer concisely using the provided context. If unknown, say you don't know.\"\n",
    "    prompt_i = f\"{sys_prompt}\\nContext: {ctx_text}\\nQuestion: {q}\"\n",
    "    pred_answers_enh.append(generate_answer(prompt_i, max_new_tokens=MAX_NEW_TOKENS))\n",
    "    contexts_list_enh.append(contexts)\n",
    "    confidences.append(conf_score)\n",
    "\n",
    "enhanced_df = subset_q[[\"question\", \"answer\"]].copy()\n",
    "enhanced_df[\"pred_answer\"] = pred_answers_enh\n",
    "enhanced_df[\"contexts_list\"] = contexts_list_enh\n",
    "enhanced_df[\"confidence\"] = confidences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63233001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enhanced_subset': 100, 'em': 57.99999999999999, 'f1': 62.73079050579051}\n"
     ]
    }
   ],
   "source": [
    "# 3) EM/F1 on enhanced subset\n",
    "try:\n",
    "    preds_ = enhanced_df[\"pred_answer\"].astype(str).tolist()\n",
    "    refs_ = enhanced_df[\"answer\"].astype(str).tolist()\n",
    "    em_enh = sum(exact_match_score(p, r) for p, r in zip(preds_, refs_)) / len(refs_) * 100.0\n",
    "    f1_enh = sum(f1_score(p, r) for p, r in zip(preds_, refs_)) / len(refs_) * 100.0\n",
    "except Exception:\n",
    "    # Fallback in case helper functions are not in scope\n",
    "    import re, string\n",
    "    def _normalize_answer(s: str) -> str:\n",
    "        def remove_articles(text): return re.sub(r\"\\b(a|an|the)\\b\", \" \", text, flags=re.IGNORECASE)\n",
    "        def remove_punc(text): return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        def white_space_fix(text): return \" \".join(text.split())\n",
    "        return white_space_fix(remove_articles(remove_punc(s.lower())))\n",
    "    def _em(a, b): return int(_normalize_answer(a) == _normalize_answer(b))\n",
    "    def _f1(a, b):\n",
    "        p = _normalize_answer(a).split(); t = _normalize_answer(b).split()\n",
    "        if not p and not t: return 100.0\n",
    "        if not p or not t: return 0.0\n",
    "        overlap = 0\n",
    "        for tok in set(p): overlap += min(p.count(tok), t.count(tok))\n",
    "        if overlap == 0: return 0.0\n",
    "        precision = overlap / len(p); recall = overlap / len(t)\n",
    "        return 2 * precision * recall / (precision + recall) * 100.0\n",
    "    em_enh = sum(_em(p, r) for p, r in zip(preds_, refs_)) / len(refs_) * 100.0\n",
    "    f1_enh = sum(_f1(p, r) for p, r in zip(preds_, refs_)) / len(refs_)\n",
    "\n",
    "with open(\"enhanced_results.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"embedding_model\": \"sentence-transformers/all-mpnet-base-v2\",\n",
    "        \"reranker\": RERANK_MODEL_NAME,\n",
    "        \"top_k\": TOP_K,\n",
    "        \"num_eval\": int(len(enhanced_df)),\n",
    "        \"em\": float(em_enh),\n",
    "        \"f1\": float(f1_enh)\n",
    "    }, f, indent=2)\n",
    "print({\"enhanced_subset\": len(enhanced_df), \"em\": em_enh, \"f1\": f1_enh})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ac729a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:00<00:00, 10609.90 examples/s]\n",
      "Evaluating:  22%|██▏       | 88/400 [00:13<01:02,  5.02it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  22%|██▎       | 90/400 [00:13<00:55,  5.54it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  24%|██▍       | 96/400 [00:14<00:33,  9.12it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  36%|███▋      | 145/400 [00:21<00:35,  7.09it/s]Failed to parse output. Returning None.\n",
      "Failed to parse output. Returning None.\n",
      "Evaluating:  38%|███▊      | 154/400 [00:23<00:56,  4.34it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  46%|████▌     | 184/400 [00:28<00:50,  4.28it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  51%|█████▏    | 205/400 [00:30<00:38,  5.12it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  56%|█████▌    | 223/400 [00:33<00:41,  4.31it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  63%|██████▎   | 251/400 [00:36<00:16,  8.98it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  64%|██████▍   | 257/400 [00:38<00:18,  7.76it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  67%|██████▋   | 269/400 [00:42<00:31,  4.19it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  94%|█████████▍| 378/400 [01:22<00:13,  1.66it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  96%|█████████▌| 382/400 [01:23<00:06,  2.84it/s]Failed to parse output. Returning None.\n",
      "Evaluating:  97%|█████████▋| 388/400 [01:25<00:04,  2.48it/s]Failed to parse output. Returning None.\n",
      "Evaluating: 100%|██████████| 400/400 [01:55<00:00,  3.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ragas_enhanced.csv with 100 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Low-budget RAGAs for enhanced\n",
    "K = min(100, len(enhanced_df))  # smaller than naive to avoid rate limits\n",
    "if \"OPENAI_API_KEY\" in os.environ and os.environ[\"OPENAI_API_KEY\"]:\n",
    "    data_enh = {\n",
    "        \"question\": enhanced_df[\"question\"].astype(str).tolist(),\n",
    "        \"answer\": enhanced_df[\"pred_answer\"].astype(str).tolist(),\n",
    "        \"contexts\": enhanced_df[\"contexts_list\"].tolist(),\n",
    "        \"ground_truth\": enhanced_df[\"answer\"].astype(str).tolist(),\n",
    "    }\n",
    "    ds_enh = Dataset.from_dict(data_enh)\n",
    "    rng = np.random.default_rng(123)\n",
    "    idx = rng.choice(len(ds_enh), size=K, replace=False)\n",
    "    subset_enh = ds_enh.select(idx.tolist())\n",
    "\n",
    "    def _trim(row, max_chars=600):\n",
    "        row[\"contexts\"] = [c[:max_chars] for c in row.get(\"contexts\", [])]\n",
    "        return row\n",
    "    subset_enh = subset_enh.map(_trim)\n",
    "\n",
    "    ragas_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=96, api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "    rc = RunConfig(timeout=120)\n",
    "    metrics = [faithfulness, answer_relevancy, context_precision, context_recall]\n",
    "\n",
    "    res = ragas_evaluate(subset_enh, metrics=metrics, llm=ragas_llm, run_config=rc)\n",
    "    df_enh = res.to_pandas()\n",
    "    df_enh.to_csv(\"ragas_enhanced.csv\", index=False)\n",
    "    print(f\"Saved ragas_enhanced.csv with {len(df_enh)} rows\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set; skipping RAGAs for enhanced.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bda2f79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison_analysis.csv and enhanced_results_summary.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>naive</th>\n",
       "      <th>enhanced</th>\n",
       "      <th>delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>faithfulness</td>\n",
       "      <td>0.590426</td>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.168398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>answer_relevancy</td>\n",
       "      <td>0.789511</td>\n",
       "      <td>0.830987</td>\n",
       "      <td>0.041476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>context_precision</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>context_recall</td>\n",
       "      <td>0.525253</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.284747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              metric     naive  enhanced     delta\n",
       "0       faithfulness  0.590426  0.758824  0.168398\n",
       "1   answer_relevancy  0.789511  0.830987  0.041476\n",
       "2  context_precision  0.570000  0.890000  0.320000\n",
       "3     context_recall  0.525253  0.810000  0.284747"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results summary — compare naive vs enhanced (saved to current directory)\n",
    "import pandas as pd, numpy as np, json, os\n",
    "\n",
    "naive = pd.read_csv(\"ragas_naive.csv\")\n",
    "enh   = pd.read_csv(\"ragas_enhanced.csv\") if os.path.exists(\"ragas_enhanced.csv\") else pd.DataFrame()\n",
    "cols = [\"faithfulness\",\"answer_relevancy\",\"context_precision\",\"context_recall\"]\n",
    "\n",
    "def means(df):\n",
    "    return {c: float(np.nanmean(df[c])) for c in cols if c in df.columns and len(df) > 0}\n",
    "\n",
    "naive_mean = means(naive)\n",
    "enh_mean   = means(enh)\n",
    "\n",
    "cmp = pd.DataFrame([\n",
    "    {\"metric\": c, \"naive\": naive_mean.get(c), \"enhanced\": enh_mean.get(c),\n",
    "     \"delta\": (enh_mean.get(c, np.nan) - naive_mean.get(c, np.nan)) if enh_mean.get(c) is not None else None}\n",
    "    for c in cols\n",
    "])\n",
    "\n",
    "cmp.to_csv(\"comparison_analysis.csv\", index=False)\n",
    "with open(\"enhanced_results_summary.json\",\"w\") as f:\n",
    "    json.dump({\"naive\": naive_mean, \"enhanced\": enh_mean,\n",
    "               \"delta\": {r[\"metric\"]: r[\"delta\"] for _, r in cmp.iterrows()}}, f, indent=2)\n",
    "\n",
    "print(\"Saved comparison_analysis.csv and enhanced_results_summary.json\")\n",
    "cmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598891d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/Users/yerouwang/Library/Python/3.9/lib/python/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved parameter_comparison.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>em</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1</td>\n",
       "      <td>55.833333</td>\n",
       "      <td>60.402643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>3</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>42.286878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>21.883684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>59.355024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>3</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>41.931178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           embedding_model  top_k         em         f1\n",
       "0   sentence-transformers/all-MiniLM-L6-v2      1  55.833333  60.402643\n",
       "1   sentence-transformers/all-MiniLM-L6-v2      3  38.333333  42.286878\n",
       "2   sentence-transformers/all-MiniLM-L6-v2      5  19.166667  21.883684\n",
       "3  sentence-transformers/all-mpnet-base-v2      1  55.000000  59.355024\n",
       "4  sentence-transformers/all-mpnet-base-v2      3  38.333333  41.931178"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Parameter comparison (2 embeddings × top_k 3/5/10) with EM/F1\n",
    "from sentence_transformers import SentenceTransformer as ST\n",
    "import pandas as pd\n",
    "\n",
    "EMBED_MODELS = [\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\",  # 384\n",
    "    \"sentence-transformers/all-mpnet-base-v2\", # 768\n",
    "]\n",
    "COLLECTION_BY_MODEL = {\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\": \"rag_mini\",\n",
    "    \"sentence-transformers/all-mpnet-base-v2\": \"rag_mini_mpnet\",\n",
    "}\n",
    "TOP_KS = [1, 3, 5]\n",
    "CAND_LIMIT = 30\n",
    "MAX_NEW_TOKENS = 24\n",
    "\n",
    "subset_for_grid = clean_queries.head(120).copy()  # keep it light but >=100\n",
    "\n",
    "results_rows = []\n",
    "for model_name in EMBED_MODELS:\n",
    "    emb_model = ST(model_name)\n",
    "    collection = COLLECTION_BY_MODEL[model_name]\n",
    "    for q in subset_for_grid[\"question\"].astype(str).tolist():\n",
    "        q_emb = emb_model.encode([q], normalize_embeddings=True)\n",
    "        sres = client.search(\n",
    "            collection_name=collection,\n",
    "            data=q_emb.tolist(),\n",
    "            anns_field=\"embedding\",\n",
    "            limit=CAND_LIMIT,\n",
    "            search_params={\"metric_type\": \"IP\"}\n",
    "        )\n",
    "        hits_local = sres[0] if len(sres) > 0 else []\n",
    "        cand_ids = [int(h[\"id\"]) for h in hits_local]\n",
    "        cand_texts = [\n",
    "            passages_reset.loc[passages_reset[\"id\"]==cid, \"passage\"].values[0]\n",
    "            for cid in cand_ids\n",
    "        ] if len(cand_ids) > 0 else []\n",
    "\n",
    "        # simple rerank to stabilize order across top_k\n",
    "        if len(cand_ids) > 0:\n",
    "            _, _, id_prob_list = rerank_with_confidence(q, cand_ids, cand_texts)\n",
    "            sorted_ids = [cid for cid,_ in sorted(id_prob_list, key=lambda x:x[1], reverse=True)]\n",
    "        else:\n",
    "            sorted_ids = []\n",
    "\n",
    "        for k in TOP_KS:\n",
    "            topk_ids = sorted_ids[:k]\n",
    "            ctxs = [passages_reset.loc[passages_reset[\"id\"]==cid, \"passage\"].values[0] for cid in topk_ids] if len(topk_ids)>0 else [\"\"]\n",
    "            ctx_text = \"\\n\\n\".join(ctxs)\n",
    "            sys_prompt = \"You are a helpful assistant. Answer concisely using the provided context. If unknown, say you don't know.\"\n",
    "            prompt_i = f\"{sys_prompt}\\nContext: {ctx_text}\\nQuestion: {q}\"\n",
    "            pred = generate_answer(prompt_i, max_new_tokens=MAX_NEW_TOKENS)\n",
    "            results_rows.append({\n",
    "                \"embedding_model\": model_name,\n",
    "                \"top_k\": k,\n",
    "                \"question\": q,\n",
    "                \"pred\": pred,\n",
    "            })\n",
    "\n",
    "# assemble and score\n",
    "res_df = pd.DataFrame(results_rows)\n",
    "merged = subset_for_grid[[\"question\",\"answer\"]].merge(res_df, on=\"question\", how=\"right\")\n",
    "\n",
    "scores = []\n",
    "for (m, k), g in merged.groupby([\"embedding_model\",\"top_k\"]):\n",
    "    preds = g[\"pred\"].astype(str).tolist()\n",
    "    refs = g[\"answer\"].astype(str).tolist()\n",
    "    em_ = sum(exact_match_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "    f1_ = sum(f1_score(p, r) for p, r in zip(preds, refs)) / len(refs) * 100.0\n",
    "    scores.append({\"embedding_model\": m, \"top_k\": k, \"em\": em_, \"f1\": f1_})\n",
    "\n",
    "param_cmp = pd.DataFrame(scores).sort_values([\"embedding_model\",\"top_k\"]).reset_index(drop=True)\n",
    "param_cmp.to_csv(\"parameter_comparison.csv\", index=False)\n",
    "print(\"Saved parameter_comparison.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e07dbb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embedding_model</th>\n",
       "      <th>top_k</th>\n",
       "      <th>em</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>1</td>\n",
       "      <td>55.833333</td>\n",
       "      <td>60.402643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>3</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>42.286878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sentence-transformers/all-MiniLM-L6-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>21.883684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>1</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>59.355024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>3</td>\n",
       "      <td>38.333333</td>\n",
       "      <td>41.931178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sentence-transformers/all-mpnet-base-v2</td>\n",
       "      <td>5</td>\n",
       "      <td>19.166667</td>\n",
       "      <td>21.295841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           embedding_model  top_k         em         f1\n",
       "0   sentence-transformers/all-MiniLM-L6-v2      1  55.833333  60.402643\n",
       "1   sentence-transformers/all-MiniLM-L6-v2      3  38.333333  42.286878\n",
       "2   sentence-transformers/all-MiniLM-L6-v2      5  19.166667  21.883684\n",
       "3  sentence-transformers/all-mpnet-base-v2      1  55.000000  59.355024\n",
       "4  sentence-transformers/all-mpnet-base-v2      3  38.333333  41.931178\n",
       "5  sentence-transformers/all-mpnet-base-v2      5  19.166667  21.295841"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_cmp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
